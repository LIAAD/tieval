{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497ade9f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# tieval: An Overview\n",
    "\n",
    "This notebook is intended to provide an introductory overview of the tieval framework. tieval is a python library that was build to mitigate the issues on the development of temporal information extraction systems.\n",
    "\n",
    "<img src=\"../../imgs/tieval.png\" alt=\"tieval logo\" align=\"center\" width=\"500\" />\n",
    "\n",
    "Let's dive into it.\n",
    "\n",
    "The first step to use tieval is to install it. In order to facilitate that, we published tieval on [python package index](https://pypi.org/) so to install it, one just needs to run the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807769c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install tieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5205abd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "tieval is divided into three main module - namely: datasets, models, and evaluation - that are intended to accommodate the full cycle of the machine learning project. In the following sections we will give a tour over the three modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae99454",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Datasets\n",
    "\n",
    "The datasets' module is responsible for downloading and reading corpus annotated with temporal information. The corpus supported by `tieval` are listed in the `SUPPORTED_DATASETS` list. The script bellow prints the name of the currently supported corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614c8e5c-d9f6-4591-9110-e9f316163b44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets: 46\n",
      "Dataset names:\n",
      "\t ancient_time_arabic\n",
      "\t ancient_time_dutch\n",
      "\t ancient_time_english\n",
      "\t ancient_time_french\n",
      "\t ancient_time_german\n",
      "\t ancient_time_italian\n",
      "\t ancient_time_spanish\n",
      "\t ancient_time_vietnamese\n",
      "\t aquaint\n",
      "\t eventtime\n",
      "\t fr_timebank\n",
      "\t grapheve\n",
      "\t krauts\n",
      "\t krauts_diezeit\n",
      "\t krauts_dolomiten_42\n",
      "\t krauts_dolomiten_100\n",
      "\t matres\n",
      "\t meantime_english\n",
      "\t meantime_spanish\n",
      "\t meantime_dutch\n",
      "\t meantime_italian\n",
      "\t narrative_container\n",
      "\t ph_english\n",
      "\t ph_french\n",
      "\t ph_german\n",
      "\t ph_italian\n",
      "\t ph_portuguese\n",
      "\t ph_spanish\n",
      "\t platinum\n",
      "\t spanish_timebank\n",
      "\t tcr\n",
      "\t tddiscourse\n",
      "\t tempeval_2_chinese\n",
      "\t tempeval_2_english\n",
      "\t tempeval_2_french\n",
      "\t tempeval_2_italian\n",
      "\t tempeval_2_korean\n",
      "\t tempeval_2_spanish\n",
      "\t tempeval_3\n",
      "\t timebank_1.2\n",
      "\t timebank_dense\n",
      "\t timebankpt\n",
      "\t timebank\n",
      "\t traint3\n",
      "\t wikiwars\n",
      "\t wikiwars_de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from tieval import datasets\n",
    "\n",
    "n_datasets = len(datasets.SUPPORTED_DATASETS)\n",
    "print(f\"Total number of datasets: {n_datasets}\")\n",
    "\n",
    "print(f\"Dataset names:\")\n",
    "for dataset_name in datasets.SUPPORTED_DATASETS:\n",
    "    print(\"\\t\", dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ace0ba-a3e8-4747-a09c-ca9157253dda",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download and read dataset\n",
    "\n",
    "Previous to the release of tieval, one interested in leveraging corpus annotates with temporal information would have to develop software to read the corpus. Furthermore, since most of the corpus are stored in different formats, he/she would have to tailor a corpus reader for each of the corpus. This accumulates to significant engineering endeavour that typically limit the amount of corpus employed in the development of the systems.\n",
    "\n",
    "tieval provides the datasets readers out-of-the-box for the corpus listed above. The script bellow exemplifies how one can read the TempEval-3 corpus with tieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c01a25a-63a9-4d30-8f7e-f0ca0778ebd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/275 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:01<00:00, 232.21it/s]\n"
     ]
    }
   ],
   "source": [
    "te3 = datasets.read(\"tempeval_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f546f5-5f46-438e-ba44-bea6001764a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By running the previous script, tieval creates a data/ folder at the root of the project directory, and downloads the TempEval-3 annotated corpus to there. After that, the `read()` function parses the corpus with the appropriate reader and outputs an instance of the `Dataset` object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d285b04-f64d-4429-8841-97a81946c894",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### `Dataset` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee37660a-7a88-4a9d-b412-531e942abe3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tieval.base.Dataset'>\n",
      "Dataset(name=tempeval_3)\n"
     ]
    }
   ],
   "source": [
    "print(type(te3))\n",
    "print(te3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845eb529-a225-46e4-9568-5a6164b7fb12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "tempeval_3\n",
      "---\n",
      "Training Documents:\n",
      "[Document(name=APW19991024.0075), Document(name=APW19990206.0090), Document(name=APW19980213.1310), Document(name=NYT19981026.0446), Document(name=wsj_0781), Document(name=XIE19980812.0062), Document(name=wsj_1073), Document(name=wsj_0973), Document(name=APW20000328.0257), Document(name=APW19980807.0261), Document(name=wsj_0685), Document(name=wsj_0811), Document(name=wsj_0713), Document(name=APW20000401.0150), Document(name=APW19991008.0265), Document(name=wsj_0159), Document(name=XIE19990210.0079), Document(name=wsj_1014), Document(name=wsj_0760), Document(name=CNN19980222.1130.0084), Document(name=wsj_0348), Document(name=APW19990507.0207), Document(name=PRI19980213.2000.0313), Document(name=NYT20000105.0325), Document(name=wsj_0928), Document(name=wsj_0151), Document(name=wsj_0667), Document(name=XIE19990227.0171), Document(name=wsj_0106), Document(name=wsj_0585), Document(name=AP900816-0139), Document(name=wsj_0184), Document(name=wsj_0356), Document(name=wsj_0950), Document(name=wsj_0584), Document(name=ea980120.1830.0071), Document(name=wsj_0158), Document(name=wsj_0938), Document(name=XIE19981203.0008), Document(name=VOA19980331.1700.1533), Document(name=ed980111.1130.0089), Document(name=NYT19980212.0019), Document(name=wsj_0924), Document(name=PRI19980205.2000.1890), Document(name=APW19980930.0425), Document(name=SJMN91-06338157), Document(name=wsj_0533), Document(name=CNN19980213.2130.0155), Document(name=NYT19990419.0515), Document(name=APW20000216.0193), Document(name=NYT20000330.0406), Document(name=wsj_1011), Document(name=NYT20000414.0296), Document(name=wsj_0762), Document(name=wsj_0026), Document(name=APW20000417.0031), Document(name=wsj_0187), Document(name=wsj_0778), Document(name=wsj_0575), Document(name=APW19991008.0151), Document(name=APW19980227.0476), Document(name=APW20000124.0182), Document(name=XIE19990313.0229), Document(name=VOA19980305.1800.2603), Document(name=wsj_0175), Document(name=wsj_0189), Document(name=wsj_0752), Document(name=wsj_0168), Document(name=APW19980213.1380), Document(name=wsj_0527), Document(name=wsj_0551), Document(name=NYT19980206.0460), Document(name=PRI19980121.2000.2591), Document(name=XIE19980808.0031), Document(name=wsj_0124), Document(name=NYT19990505.0443), Document(name=wsj_0340), Document(name=PRI19980303.2000.2550), Document(name=APW19980811.0474), Document(name=wsj_0176), Document(name=wsj_0558), Document(name=wsj_0172), Document(name=XIE19980821.0077), Document(name=wsj_0695), Document(name=wsj_0786), Document(name=wsj_0329), Document(name=XIE19980814.0294), Document(name=APW19980809.0700), Document(name=wsj_0122), Document(name=APW20000115.0031), Document(name=wsj_0167), Document(name=wsj_0670), Document(name=wsj_0675), Document(name=wsj_0612), Document(name=NYT20000424.0319), Document(name=NYT19981025.0216), Document(name=APW19980219.0476), Document(name=wsj_1031), Document(name=CNN19980223.1130.0960), Document(name=APW19980501.0480), Document(name=APW20000403.0057), Document(name=wsj_1003), Document(name=VOA19980303.1600.2745), Document(name=wsj_0346), Document(name=PRI19980205.2000.1998), Document(name=NYT19980402.0453), Document(name=XIE19990313.0031), Document(name=APW19990506.0155), Document(name=NYT19980424.0421), Document(name=APW19980820.1428), Document(name=wsj_0316), Document(name=wsj_0798), Document(name=wsj_1025), Document(name=wsj_0555), Document(name=ABC19980304.1830.1636), Document(name=NYT20000106.0007), Document(name=wsj_0904), Document(name=wsj_0679), Document(name=APW20000107.0318), Document(name=wsj_0144), Document(name=wsj_0815), Document(name=wsj_0610), Document(name=wsj_0806), Document(name=wsj_0751), Document(name=NYT19981120.0362), Document(name=NYT20000601.0442), Document(name=wsj_0171), Document(name=wsj_0006), Document(name=ABC19980108.1830.0711), Document(name=APW199980817.1193), Document(name=CNN19980227.2130.0067), Document(name=APW19980808.0022), Document(name=ABC19980114.1830.0611), Document(name=wsj_0321), Document(name=wsj_1038), Document(name=wsj_1033), Document(name=wsj_0706), Document(name=wsj_1040), Document(name=wsj_0505), Document(name=wsj_0586), Document(name=APW19980626.0364), Document(name=wsj_0534), Document(name=wsj_0923), Document(name=VOA19980501.1800.0355), Document(name=wsj_0161), Document(name=wsj_0736), Document(name=NYT20000406.0002), Document(name=NYT19990312.0271), Document(name=WSJ900813-0157), Document(name=NYT19981025.0188), Document(name=wsj_0637), Document(name=VOA19980303.1600.0917), Document(name=wsj_0557), Document(name=wsj_0027), Document(name=APW19980826.0389), Document(name=wsj_0816), Document(name=wsj_0471), Document(name=wsj_0583), Document(name=wsj_0160), Document(name=wsj_0165), Document(name=APW19980301.0720), Document(name=wsj_0662), Document(name=WSJ910225-0066), Document(name=wsj_0325), Document(name=wsj_1008), Document(name=XIE19980808.0049), Document(name=wsj_1039), Document(name=wsj_0157), Document(name=wsj_0292), Document(name=wsj_0568), Document(name=NYT19980206.0466), Document(name=ABC19980120.1830.0957), Document(name=ea980120.1830.0456), Document(name=wsj_0068), Document(name=wsj_1006), Document(name=XIE19990313.0173), Document(name=wsj_0135), Document(name=wsj_0541), Document(name=CNN19980126.1600.1104), Document(name=APW19980322.0749), Document(name=XIE19980808.0188), Document(name=APW20000106.0064), Document(name=wsj_0661), Document(name=wsj_0136), Document(name=wsj_0906), Document(name=NYT19981121.0173), Document(name=wsj_0660), Document(name=APW19980911.0475), Document(name=APW19990607.0041), Document(name=wsj_0132), Document(name=wsj_0150), Document(name=NYT20000403.0463), Document(name=APW19980813.1117), Document(name=wsj_0542), Document(name=NYT20000113.0267), Document(name=wsj_0152), Document(name=NYT20000224.0173), Document(name=wsj_0127), Document(name=APW19980227.0468), Document(name=wsj_0981), Document(name=wsj_0173), Document(name=wsj_0313), Document(name=wsj_0266), Document(name=wsj_0805), Document(name=wsj_0570), Document(name=wsj_1013), Document(name=wsj_0376), Document(name=wsj_0263), Document(name=wsj_0768), Document(name=wsj_1042), Document(name=APW19980306.1001), Document(name=wsj_0674), Document(name=wsj_0709), Document(name=AP900815-0044), Document(name=wsj_0344), Document(name=APW19980213.1320), Document(name=APW20000115.0209), Document(name=PRI19980115.2000.0186), Document(name=APW19980810.0907), Document(name=APW19990122.0193), Document(name=PRI19980306.2000.1675), Document(name=wsj_0918), Document(name=NYT20000329.0359), Document(name=APW19980418.0210), Document(name=wsj_1035), Document(name=APW20000210.0328), Document(name=wsj_0927), Document(name=APW19980227.0487), Document(name=APW19980308.0201), Document(name=wsj_0169), Document(name=wsj_0520), Document(name=APW20000128.0316), Document(name=wsj_0032), Document(name=wsj_0073), Document(name=wsj_0332), Document(name=APW19981205.0374), Document(name=APW19980227.0494), Document(name=wsj_0991), Document(name=wsj_0745), Document(name=APW20000216.0272), Document(name=APW19990410.0123), Document(name=APW20000405.0276), Document(name=APW19980227.0489), Document(name=wsj_0791), Document(name=APW19980818.0515), Document(name=XIE19980809.0010), Document(name=APW19990216.0198), Document(name=PRI19980216.2000.0170), Document(name=wsj_0650), Document(name=wsj_0907), Document(name=wsj_0324), Document(name=APW19990312.0251), Document(name=XIE19980808.0060), Document(name=NYT19980907.0112), Document(name=wsj_0810)]\n",
      "---\n",
      "Test Documents:\n",
      "[Document(name=CNN_20130322_1003), Document(name=nyt_20130321_china_pollution), Document(name=nyt_20130321_cyprus), Document(name=WSJ_20130318_731), Document(name=WSJ_20130321_1145), Document(name=CNN_20130322_314), Document(name=nyt_20130321_sarkozy), Document(name=bbc_20130322_1600), Document(name=nyt_20130321_women_senate), Document(name=CNN_20130322_1243), Document(name=bbc_20130322_332), Document(name=nyt_20130322_strange_computer), Document(name=WSJ_20130322_159), Document(name=CNN_20130321_821), Document(name=bbc_20130322_1353), Document(name=CNN_20130322_248), Document(name=WSJ_20130322_804), Document(name=bbc_20130322_721), Document(name=bbc_20130322_1150), Document(name=AP_20130322)]\n"
     ]
    }
   ],
   "source": [
    "# Attributes\n",
    "print(\"Name:\")\n",
    "print(te3.name)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Training Documents:\")\n",
    "print(te3.train)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Test Documents:\")\n",
    "print(te3.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfc48c-4b8b-46b7-ba96-7005cc9f2d2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `Dataset` object is the final representation of each dataset. Within it, one can find three attributes of major interest:\n",
    "    - `.name` a string that contains the name of the datasets \n",
    "    - `.train` a list with the training documents\n",
    "    - `.test` a list with the test documents\n",
    "    \n",
    "Furthermore, one can access al the documents with the `.documents` property.\n",
    "\n",
    "> All the documents are placed in the `.train` attribute in case no train/ test is provided by the original authors of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898c4a-6bd2-4db4-8048-ca2e9db7525a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### `Document` object\n",
    "\n",
    "Each document of the corpus is sorted as an instance of a `Document` which compiles the document level annotation. Most importantly, it contains the temporal entities and the temporal links that were annotated on the `.entities` and `.tlinks` attributes.  To demonstrate this we will use document **wsj_0006** of TempEval-3 as it the shortest document of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfc0cc2-2c4b-4c59-9608-065a4064aeb6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'tieval.base.Document'>\n",
      "Pacific First Financial Corp. said shareholders approved its acquisition by Royal Trustco Ltd. of Toronto for $27 a share, or $212 million.\n",
      "The thrift holding company said it expects to obtain regulatory approval and complete the transaction by year-end.\n"
     ]
    }
   ],
   "source": [
    "doc_name = \"wsj_0006\"\n",
    "doc = te3[doc_name]\n",
    "print(\"Type:\", type(doc))\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5035005d-7b5a-4484-ac6c-0961e08e7b4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:\n",
      "{Event(\"acquisition\"), Event(\"approved\"), Event(\"said\"), Event(\"said\"), Event(\"obtain\"), Timex(\"year-end\"), Event(\"transaction\"), Event(\"complete\"), Timex(\"11/02/89\"), Event(\"approval\"), Event(\"expects\")}\n",
      "---\n",
      "TLinks:\n",
      "{TLink(Event(\"complete\") ---BEFORE--> Timex(\"year-end\")), TLink(Event(\"said\") ---SIMULTANEOUS--> Event(\"said\")), TLink(Event(\"complete\") ---ENDS--> Event(\"transaction\")), TLink(Event(\"said\") ---BEFORE--> Timex(\"11/02/89\")), TLink(Event(\"approved\") ---AFTER--> Event(\"acquisition\")), TLink(Event(\"approved\") ---BEFORE--> Event(\"said\"))}\n"
     ]
    }
   ],
   "source": [
    "print(\"Entities:\")\n",
    "print(doc.entities)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"TLinks:\")\n",
    "print(doc.tlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9eaa3-b2f1-435f-b882-2acbd5e58190",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the entities can take two types: `Event` or `Timex`. This goe sin accordance to the traditional approach of temporal information extraction in which the temporal entities take this two types.\n",
    "\n",
    "Other attributes that can be found in a `Document` instance are: \n",
    "    - `.name` a string with the name of the document\n",
    "    - `.text` a string with the text of the document\n",
    "    - `.dct` a timex that represents the document creation time\n",
    "    - `.events` the set of annotated events\n",
    "    - `.timexs` the set of annotated timexs\n",
    "\n",
    "The script bellow presents those attributes for our reference corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a924275f-c851-4ef0-94e7-e30b9655d9c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "wsj_0006\n",
      "---\n",
      "Text:\n",
      "Pacific First Financial Corp. said shareholders approved its acquisition by Royal Trustco Ltd. of Toronto for $27 a share, or $212 million.\n",
      "The thrift holding company said it expects to obtain regulatory approval and complete the transaction by year-end.\n",
      "---\n",
      "Document Creation Time:\n",
      "Timex(\"11/02/89\")\n",
      "---\n",
      "Events:\n",
      "{Event(\"acquisition\"), Event(\"approved\"), Event(\"said\"), Event(\"said\"), Event(\"obtain\"), Event(\"transaction\"), Event(\"complete\"), Event(\"approval\"), Event(\"expects\")}\n",
      "---\n",
      "Timexs:\n",
      "{Timex(\"11/02/89\"), Timex(\"year-end\")}\n"
     ]
    }
   ],
   "source": [
    "print(\"Name:\")\n",
    "print(doc.name)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Text:\")\n",
    "print(doc.text)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Document Creation Time:\")\n",
    "print(doc.dct)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Events:\")\n",
    "print(doc.events)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Timexs:\")\n",
    "print(doc.timexs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08306fcd-2075-42e4-b0a4-c7f345747845",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Temporal Closure\n",
    "One of the main contributions of `tieval` is providing the research community an easy to compute temporal closure of a document. For more about temporal closure see [James Allen](https://dl.acm.org/doi/pdf/10.1145/182.358434) work. In `tieval` the closure operation can be performed in two ways: by calling the `.temporal_closure` property of a `Document`, or by using applying the `temporal_closure()` function to a set of TLinks, as is demonstrated on the script bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7327a928-e02f-4bb3-8b7f-4da4bcf88423",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document property:\n",
      "{TLink(ei76 ---BEFORE--> t0), TLink(ei74 ---BEFORE--> t0), TLink(ei74 ---BEFORE--> ei76), TLink(ei80 ---BEFORE--> t10), TLink(ei75 ---BEFORE--> t0), TLink(ei73 ---SIMULTANEOUS--> ei76), TLink(ei81 ---BEFORE--> t10), TLink(ei80 ---ENDS--> ei81), TLink(ei73 ---AFTER--> ei75), TLink(ei73 ---BEFORE--> t0), TLink(ei75 ---BEFORE--> ei76), TLink(ei74 ---AFTER--> ei75), TLink(ei73 ---AFTER--> ei74)}\n",
      "---\n",
      "Closure function:\n",
      "{TLink(ei76 ---BEFORE--> t0), TLink(ei74 ---BEFORE--> t0), TLink(ei74 ---BEFORE--> ei76), TLink(ei80 ---BEFORE--> t10), TLink(ei75 ---BEFORE--> t0), TLink(ei73 ---SIMULTANEOUS--> ei76), TLink(ei81 ---BEFORE--> t10), TLink(ei80 ---ENDS--> ei81), TLink(ei73 ---AFTER--> ei75), TLink(ei73 ---BEFORE--> t0), TLink(ei75 ---BEFORE--> ei76), TLink(ei74 ---AFTER--> ei75), TLink(ei73 ---AFTER--> ei74)}\n",
      "---\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Document property:\")\n",
    "tlinks_closure_doc = doc.temporal_closure\n",
    "print(tlinks_closure_doc)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Closure function:\")\n",
    "from tieval.closure import temporal_closure\n",
    "tlinks_closure_func = temporal_closure(doc.tlinks)\n",
    "print(tlinks_closure_func)\n",
    "\n",
    "print(\"---\")\n",
    "print(tlinks_closure_doc == tlinks_closure_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cc8dc-5950-4b9b-9230-9c1e3ad67fe1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Statistics\n",
    "\n",
    "Now we can produce a script that presents the number of documents, events, timexs, and tlinks for each corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "272fa976-e02c-4744-bcc6-723a0adbe6ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_arabic...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 818.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_dutch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1663.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_english...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 658.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_french...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 635.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_german...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1036.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_italian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5/5 [00:00<00:00, 649.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_spanish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 914.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ancient_time_vietnamese...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 644.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset aquaint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 126.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset eventtime...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 318.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset fr_timebank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:00<00:00, 665.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset grapheve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:24<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset krauts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [00:00<00:00, 3375.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset krauts_diezeit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 2321.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset krauts_dolomiten_42...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 3651.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset krauts_dolomiten_100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 3886.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset matres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:01<00:00, 238.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset meantime_english...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 155.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset meantime_spanish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 180.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset meantime_dutch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 204.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset meantime_italian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 181.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset narrative_container...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ph_english...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24642/24642 [00:03<00:00, 7195.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ph_french...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27154/27154 [00:02<00:00, 12013.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ph_german...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19095/19095 [00:02<00:00, 7287.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ph_italian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9619/9619 [00:01<00:00, 9267.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ph_portuguese...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24293/24293 [00:02<00:00, 11155.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset ph_spanish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33266/33266 [00:04<00:00, 6865.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset platinum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 229.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset spanish_timebank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:00<00:00, 279.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tcr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 182.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tddiscourse...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 254.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_2_chinese...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:00<00:00, 143.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_2_english...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:00<00:00, 271.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_2_french...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:00<00:00, 886.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_2_italian...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 306.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_2_korean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 70.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_2_spanish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [00:00<00:00, 269.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset tempeval_3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:01<00:00, 213.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset timebank_1.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 246.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset timebank_dense...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 320.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset timebankpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:00<00:00, 372.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset timebank...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [00:00<00:00, 289.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset traint3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [00:00<00:00, 186.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset wikiwars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 195.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset wikiwars_de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 247.53it/s]\n"
     ]
    }
   ],
   "source": [
    "statistics = []\n",
    "for dataset_name in datasets.SUPPORTED_DATASETS:\n",
    "    \n",
    "    print(f\"Reading dataset {dataset_name}...\")\n",
    "    dataset = datasets.read(dataset_name)\n",
    "    \n",
    "    n_docs = len(dataset.documents)\n",
    "    n_events = sum(len(doc.events) for doc in dataset.documents)\n",
    "    n_timexs = sum(len(doc.timexs) for doc in dataset.documents)\n",
    "    n_tlinks = sum(len(doc.tlinks) for doc in dataset.documents if doc.tlinks)\n",
    "    statistics += [(dataset.name, n_docs, n_events, n_timexs, n_tlinks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f741862a-bca5-4e79-a2bf-44716396f4b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\t\t\t#docs\t#events\t#timexs\t#tlinks\n",
      "-------------------------------------------------------\n",
      "Ancient_time_arabic \t5\t0\t106\t0\n",
      "Ancient_time_dutch  \t5\t0\t130\t0\n",
      "Ancient_time_english\t5\t0\t311\t0\n",
      "Ancient_time_french \t5\t0\t290\t0\n",
      "Ancient_time_german \t5\t0\t196\t0\n",
      "Ancient_time_italian\t5\t0\t234\t0\n",
      "Ancient_time_spanish\t5\t0\t217\t0\n",
      "Ancient_time_vietnamese\t4\t0\t120\t0\n",
      "Aquaint             \t72\t4351\t639\t5832\n",
      "Eventtime           \t36\t1498\t0\t0\n",
      "Fr_timebank         \t108\t2115\t533\t2303\n",
      "Grapheve            \t103\t4298\t0\t18204\n",
      "Krauts              \t192\t0\t1282\t0\n",
      "Krauts_diezeit      \t50\t0\t553\t0\n",
      "Krauts_dolomiten_42 \t42\t0\t228\t0\n",
      "Krauts_dolomiten_100\t100\t0\t501\t0\n",
      "Matres              \t274\t6065\t0\t13504\n",
      "Meantime_english    \t120\t1882\t349\t1753\n",
      "Meantime_spanish    \t120\t2000\t344\t1975\n",
      "Meantime_dutch      \t120\t1346\t346\t1487\n",
      "Meantime_italian    \t120\t1980\t338\t1675\n",
      "Narrative_container \t63\t3559\t439\t737\n",
      "Ph_english          \t24642\t0\t254803\t0\n",
      "Ph_french           \t27154\t0\t83431\t0\n",
      "Ph_german           \t19095\t0\t194043\t0\n",
      "Ph_italian          \t9619\t0\t58823\t0\n",
      "Ph_portuguese       \t24293\t0\t111810\t0\n",
      "Ph_spanish          \t33266\t0\t348011\t0\n",
      "Platinum            \t20\t748\t158\t929\n",
      "Spanish_timebank    \t210\t12384\t1532\t21107\n",
      "Tcr                 \t25\t1134\t242\t3515\n",
      "Tddiscourse         \t34\t1101\t0\t6150\n",
      "Tempeval_2_chinese  \t52\t4783\t946\t7802\n",
      "Tempeval_2_english  \t182\t6656\t1390\t5945\n",
      "Tempeval_2_french   \t83\t1301\t367\t372\n",
      "Tempeval_2_italian  \t64\t5377\t653\t6884\n",
      "Tempeval_2_korean   \t18\t2583\t317\t0\n",
      "Tempeval_2_spanish  \t210\t12384\t1502\t13304\n",
      "Tempeval_3          \t275\t11780\t2223\t11881\n",
      "Timebank_1.2        \t183\t7940\t1414\t6413\n",
      "Timebank_dense      \t36\t1712\t289\t12715\n",
      "Timebankpt          \t182\t7887\t1409\t6538\n",
      "Timebank            \t183\t6681\t1426\t5120\n",
      "Traint3             \t175\t10686\t1269\t17283\n",
      "Wikiwars            \t22\t0\t2662\t0\n",
      "Wikiwars_de         \t22\t0\t2239\t0\n"
     ]
    }
   ],
   "source": [
    "print(\"Name\\t\\t\\t#docs\\t#events\\t#timexs\\t#tlinks\")\n",
    "print(\"-\" * 55)\n",
    "for name, n_docs, n_events, n_timexs, n_tlinks in statistics:\n",
    "    print(f\"{name.capitalize():20}\\t{n_docs}\\t{n_events}\\t{n_timexs}\\t{n_tlinks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d5bdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models\n",
    "\n",
    "Now that we presented how to import the data we move to the second step of the machine learning pipeline: the development of the model.\n",
    "\n",
    "In its current version, `tieval` provides four models, namely:\n",
    "\n",
    "    - TimexIdentificationBaseline -> which is the NER recognition model from SpaCy that was trained to identify timexs\n",
    "    - HeidelTime -> a well known model for timex identification\n",
    "    - EventIdentificationBaseline -> the same architecture of TimexIdentificationBaseline but trained to identify event expressions\n",
    "    - CogCompTime2 -> a baseline for temporal relation classificaiton proposed by \n",
    "\n",
    "For the purpose of this notebook we will focus on timex identification, that task in which we can compare the two baseline models TimexIdentificationBaseline and HeidelTime.\n",
    "\n",
    "\n",
    "> Note: for the implementation of HeidelTime, tieval uses the python implementation [py_heidetime](https://github.com/JMendes1995/py_heideltime) repository. As this is a python wrapper of the [original implementation](https://github.com/HeidelTime/heideltime) in Java, it requires some specif installation steps. For that, please refer to the [py_heidetime](https://github.com/JMendes1995/py_heideltime) repository.\n",
    "\n",
    "The script bellow shows how to load the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97891d4a-64f7-476b-8968-1b5e89026eb5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Entity Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a0c95-967f-46fd-992e-f683273b509a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tieval import models\n",
    "\n",
    "baseline = models.TimexIdentificationBaseline()\n",
    "heideltime = models.HeidelTime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a0a76-8689-4022-8cf4-02957d9a1b3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the script above will create a `models` directory on the project root. This is where tieval will store the parameters required for the TimexIdentificationBaseline model.\n",
    "\n",
    "To get predictions from the models one just needs to pass a set of documents to the `.predict()` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c34544-3b9d-4398-9076-d68564425057",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preforming predictions with baseline model...\n",
      "Done.\n",
      "Preforming predictions with HeidelTime model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 20/20 [00:38<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preforming predictions with baseline model...\")\n",
    "baseline_predictions = baseline.predict(te3.test)\n",
    "print(\"Done.\")      \n",
    "      \n",
    "print(\"Preforming predictions with HeidelTime model...\")\n",
    "heideltime_predictions = heideltime.predict(te3.test)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa9dc6-827b-4dd3-937d-6b6d12531257",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output of the prediction method is a dictionary that maps the name of the document to the set of predicted timexs, as illustrated bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30536363-3f64-491c-8353-d7686ce3ad79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions type:\n",
      "<class 'dict'>\n",
      "---\n",
      "Predictions keys:\n",
      "dict_keys(['CNN_20130322_1003', 'CNN_20130322_248', 'CNN_20130321_821', 'CNN_20130322_314', 'AP_20130322', 'WSJ_20130318_731', 'bbc_20130322_1150', 'WSJ_20130321_1145', 'CNN_20130322_1243', 'WSJ_20130322_159', 'bbc_20130322_332', 'nyt_20130322_strange_computer', 'nyt_20130321_sarkozy', 'WSJ_20130322_804', 'bbc_20130322_1600', 'bbc_20130322_1353', 'nyt_20130321_china_pollution', 'nyt_20130321_cyprus', 'nyt_20130321_women_senate', 'bbc_20130322_721'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions type:\")\n",
    "print(type(baseline_predictions))\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Predictions keys:\")\n",
    "print(baseline_predictions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f2213-d423-4c0d-b834-2495e12030e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at the predictions made by the models to the document CNN_20130322_248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0479a-047b-449a-b86e-0e0deee3e604",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata:\n",
      "Document name: CNN_20130322_248\n",
      "Document creation time: Timex(\"March 22, 2013\")\n",
      "---\n",
      "Annotation\n",
      "{Timex(\"this fiscal year\"), Timex(\"March 22, 2013\"), Timex(\"four-week\"), Timex(\"Friday\"), Timex(\"Friday\"), Timex(\"April 7\")}\n",
      "---\n",
      "Baseline Predictions:\n",
      "[Timex(\"Friday\"), Timex(\"April 7\"), Timex(\"Friday\"), Timex(\"this fiscal year\")]\n",
      "---\n",
      "HeidelTime Predictions:\n",
      "[Timex(\"Friday\"), Timex(\"four-week\"), Timex(\"April 7\"), Timex(\"Friday\"), Timex(\"this fiscal year\")]\n"
     ]
    }
   ],
   "source": [
    "doc_name = \"CNN_20130322_248\"\n",
    "doc = te3[doc_name]\n",
    "\n",
    "print(\"Metadata:\")\n",
    "print(f\"Document name: {doc_name}\")\n",
    "print(f\"Document creation time: {doc.dct}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Annotation\")\n",
    "print(doc.timexs)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Baseline Predictions:\")\n",
    "print(baseline_predictions[doc_name])\n",
    "\n",
    "print(\"---\")\n",
    "print(\"HeidelTime Predictions:\")\n",
    "print(heideltime_predictions[doc_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903e40f-da73-48fa-9c96-cf24e2de7ec4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An important remark is the fact that none of the models predict the Timex(\"March 22, 2013\"). This is natural as that timex is the document creation time, which is not explicit on the text and is given a priori to the system. `tieval` has built-in evaluation functions for fair comparison between the models as we will see in the evaluation section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f11f47-daae-4693-9a6b-1b28ccb64e94",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Trainable models\n",
    "\n",
    "The provided by default for the event and timex baseline models were trained on the TempEval-3 corpus. However, one can train them from scratch in any other corpus. Bellow we present a script that train the TimexIdentificationBaseline on the TempEval-2 Spanish corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb8ad6-a6d9-48a7-8340-3cb565117969",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 193/193 [00:00<00:00, 464.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number timexs (train/dev): 1163/281\n",
      "--------------------------------------------------\n",
      "Training the model.\n",
      "Epoch 1\n",
      "Losses:\tTrain 10.11214\tDev 1.61868\n",
      "--------------------------------------------------\n",
      "Epoch 2\n",
      "Losses:\tTrain 1.60296\tDev 1.00113\n",
      "--------------------------------------------------\n",
      "Epoch 3\n",
      "Losses:\tTrain 0.83664\tDev 0.53589\n",
      "--------------------------------------------------\n",
      "Epoch 4\n",
      "Losses:\tTrain 0.35485\tDev 0.34843\n",
      "--------------------------------------------------\n",
      "Epoch 5\n",
      "Losses:\tTrain 0.16431\tDev 0.24151\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # supress spacy warnings\n",
    "\n",
    "import spacy\n",
    "\n",
    "# set seeds\n",
    "random.seed(73)\n",
    "spacy.util.fix_random_seed(73)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "print(\"Reading data.\")\n",
    "data = datasets.read(\"tempeval_2_spanish\")\n",
    "\n",
    "# train/ dev split\n",
    "size = len(data.train)\n",
    "dev_size = round(size * 0.2)\n",
    "random.shuffle(data.train)\n",
    "train_docs, dev_docs = data.train[dev_size:], data.train[:dev_size]\n",
    "\n",
    "# print some statistics\n",
    "n_train_timexs = sum(len(doc.timexs) for doc in train_docs)\n",
    "n_dev_timexs = sum(len(doc.timexs) for doc in dev_docs)\n",
    "\n",
    "print(f\"Number timexs (train/dev): {n_train_timexs}/{n_dev_timexs}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Training the model.\")\n",
    "model = models.TimexIdentificationBaseline()\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    if epoch == 0:  # reset weights\n",
    "        model.fit(train_docs, dev_docs, from_scratch=True)\n",
    "    else:\n",
    "        model.fit(train_docs, dev_docs)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a25c98-9cfa-455b-8f2c-f8e3b89d1ae9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the Evaluation section we will show how one can use `tieval` to further evaluate the performace of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6b7c0-f589-4442-b638-006666a3fb6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    " ### TLink Classification\n",
    "\n",
    "The model for tlinks classification follows the same principles of the timex identification. That is, the predictions are performed by means of the `.predict()` method which takes as input a set of documents and outputs a dictionary for that maps the names of the documents to the predictions performed by the system.\n",
    "\n",
    "> Beware: CogCompTime2 model is a computationally heavy model that will take some time to run.\n",
    "\n",
    "In the script bellow we load the CogCompTime2 model - that will download all the model resources to the `models` folder - the TCR corpus - as it is one of the corpus the authors used in the original evaluation of the models - and predict the temporal relations for all documents of TCR - TCR is one of the corpus that no standard train test split is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f961f8-4fb0-4b7b-9280-1dbad0fcc4cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cct2 = models.CogCompTime2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51ebaa5-48d4-4d47-aede-a0e7c555a0d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 109.25it/s]\n"
     ]
    }
   ],
   "source": [
    "tcr = datasets.read(\"tcr\")\n",
    "cct2_predictions = cct2.predict(tcr.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fed6c-7656-41db-933f-e1b6b6571423",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can look at the results for one of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b5a55-fcca-476b-bb0d-a350fc35e39d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(name=2010.01.12.haiti.earthquake),\n",
       " Document(name=2010.01.03.japan.jal.airlines.ft),\n",
       " Document(name=2010.01.08.facebook.bra.color),\n",
       " Document(name=2010.01.12.uk.islamist.group.ban),\n",
       " Document(name=2010.01.06.tennis.qatar.federer.nadal),\n",
       " Document(name=2010.02.26.census.redistricting),\n",
       " Document(name=2010.01.07.water.justice),\n",
       " Document(name=2010.03.02.health.care),\n",
       " Document(name=2010.01.01.iran.moussavi),\n",
       " Document(name=2010.01.18.sherlock.holmes.tourism.london),\n",
       " Document(name=2010.01.12.turkey.israel),\n",
       " Document(name=2010.02.05.sotu.crowley.column),\n",
       " Document(name=2010.02.03.cross.quake.resistant.housing),\n",
       " Document(name=2010.01.02.pakistan.attacks),\n",
       " Document(name=2010.01.13.haiti.un.mission),\n",
       " Document(name=2010.01.13.google.china.exit),\n",
       " Document(name=2010.02.06.iran.nuclear),\n",
       " Document(name=2010.01.13.mexico.human.traffic.drug),\n",
       " Document(name=2010.03.22.africa.elephants.ivory.trade),\n",
       " Document(name=2010.03.02.japan.unemployment.ft),\n",
       " Document(name=2010.02.07.japan.prius.recall.ft),\n",
       " Document(name=2010.01.18.uk.israel.livni),\n",
       " Document(name=2010.01.07.winter.weather),\n",
       " Document(name=2010.03.23.how.get.headhunted),\n",
       " Document(name=2010.03.17.france.eta.policeman)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcr.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c04f34-6b12-4ec8-9e30-b9c8bc8ad8ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation\n",
      "Event(\"made\") ---AFTER--> Event(\"promoting\")\n",
      "Event(\"hoping\") ---AFTER--> Event(\"promoting\")\n",
      "Event(\"told\") ---AFTER--> Event(\"visit\")\n",
      "Event(\"think\") ---AFTER--> Event(\"meaning\")\n",
      "Event(\"lived\") ---BEFORE--> Event(\"greets\")\n",
      "Event(\"examine\") ---AFTER--> Event(\"put\")\n",
      "Event(\"think\") ---AFTER--> Event(\"put\")\n",
      "Event(\"put\") ---BEFORE--> Event(\"put\")\n",
      "Event(\"visit\") ---AFTER--> Event(\"think\")\n",
      "Event(\"examine\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"think\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"visit\") ---AFTER--> Event(\"set\")\n",
      "Event(\"seen\") ---BEFORE--> Event(\"visit\")\n",
      "Event(\"examine\") ---BEFORE--> Event(\"think\")\n",
      "Event(\"examine\") ---AFTER--> Event(\"put\")\n",
      "Event(\"promoting\") ---BEFORE--> Event(\"seen\")\n",
      "Event(\"put\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"think\") ---AFTER--> Event(\"put\")\n",
      "Event(\"visit\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"said\") ---SIMULTANEOUS--> Event(\"meaning\")\n",
      "Event(\"seen\") ---BEFORE--> Event(\"told\")\n",
      "Event(\"promoting\") ---BEFORE--> Event(\"told\")\n",
      "Event(\"set\") ---AFTER--> Event(\"lived\")\n",
      "Event(\"expecting\") ---BEFORE--> Event(\"visit\")\n",
      "Event(\"meaning\") ---BEFORE--> Event(\"contributed\")\n",
      "Event(\"adding\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"greets\") ---BEFORE--> Event(\"examine\")\n",
      "Event(\"visit\") ---BEFORE--> Event(\"adding\")\n",
      "Event(\"adding\") ---AFTER--> Event(\"meaning\")\n",
      "Event(\"put\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"fueled\") ---AFTER--> Event(\"visit\")\n",
      "Event(\"expecting\") ---AFTER--> Event(\"fueled\")\n",
      "Event(\"made\") ---BEFORE--> Event(\"hoping\")\n",
      "Event(\"think\") ---AFTER--> Event(\"adding\")\n",
      "Event(\"think\") ---BEFORE--> Event(\"said\")\n",
      "---\n",
      "CogCompTime2 Predictions:\n",
      "Event(\"made\") ---AFTER--> Event(\"promoting\")\n",
      "Event(\"hoping\") ---AFTER--> Event(\"promoting\")\n",
      "Event(\"told\") ---BEFORE--> Event(\"visit\")\n",
      "Event(\"think\") ---AFTER--> Event(\"meaning\")\n",
      "Event(\"lived\") ---BEFORE--> Event(\"greets\")\n",
      "Event(\"examine\") ---AFTER--> Event(\"put\")\n",
      "Event(\"think\") ---AFTER--> Event(\"put\")\n",
      "Event(\"put\") ---BEFORE--> Event(\"put\")\n",
      "Event(\"visit\") ---BEFORE--> Event(\"think\")\n",
      "Event(\"examine\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"think\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"visit\") ---AFTER--> Event(\"set\")\n",
      "Event(\"seen\") ---BEFORE--> Event(\"visit\")\n",
      "Event(\"examine\") ---VAGUE--> Event(\"think\")\n",
      "Event(\"examine\") ---AFTER--> Event(\"put\")\n",
      "Event(\"promoting\") ---BEFORE--> Event(\"seen\")\n",
      "Event(\"put\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"think\") ---AFTER--> Event(\"put\")\n",
      "Event(\"visit\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"said\") ---AFTER--> Event(\"meaning\")\n",
      "Event(\"seen\") ---BEFORE--> Event(\"told\")\n",
      "Event(\"promoting\") ---BEFORE--> Event(\"told\")\n",
      "Event(\"set\") ---AFTER--> Event(\"lived\")\n",
      "Event(\"expecting\") ---AFTER--> Event(\"visit\")\n",
      "Event(\"meaning\") ---BEFORE--> Event(\"contributed\")\n",
      "Event(\"adding\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"greets\") ---BEFORE--> Event(\"examine\")\n",
      "Event(\"visit\") ---BEFORE--> Event(\"adding\")\n",
      "Event(\"adding\") ---AFTER--> Event(\"meaning\")\n",
      "Event(\"put\") ---BEFORE--> Event(\"said\")\n",
      "Event(\"fueled\") ---BEFORE--> Event(\"visit\")\n",
      "Event(\"expecting\") ---AFTER--> Event(\"fueled\")\n",
      "Event(\"made\") ---BEFORE--> Event(\"hoping\")\n",
      "Event(\"think\") ---AFTER--> Event(\"adding\")\n",
      "Event(\"think\") ---BEFORE--> Event(\"said\")\n"
     ]
    }
   ],
   "source": [
    "doc = tcr.documents[9]\n",
    "\n",
    "print(\"Annotation\")\n",
    "for tlink in doc.tlinks:\n",
    "    print(tlink)\n",
    "    \n",
    "print(\"---\")\n",
    "print(\"CogCompTime2 Predictions:\")\n",
    "for tlink in cct2_predictions[doc.name]:\n",
    "    print(tlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae8dd7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Another interesting contribution of `tieval` is the evaluation infrastructure it provides. In particular, the framework provides evaluation functions for entity identification, tlink identification, and tlink classification. The tlink classification produces statists for temporal awareness, a more comprehensive metric to evaluation temporal relation extraction systems, that is not reported in many works.\n",
    "\n",
    "### Timex Evaluation\n",
    "As an example, the script bellow evaluates the predictions produced by the TimexIdentificationBaseline and the HeidelTime system that were produced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073401ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results:\n",
      "|       |    f1 |   precision |   recall |\n",
      "|-------+-------+-------------+----------|\n",
      "| macro | 0.778 |       0.817 |    0.742 |\n",
      "| micro | 0.795 |       0.851 |    0.746 |\n",
      "---\n",
      "HeidelTime Results:\n",
      "|       |    f1 |   precision |   recall |\n",
      "|-------+-------+-------------+----------|\n",
      "| macro | 0.787 |       0.811 |    0.765 |\n",
      "| micro | 0.818 |       0.84  |    0.797 |\n"
     ]
    }
   ],
   "source": [
    "from tieval import evaluate\n",
    "\n",
    "annotation_te3 = {doc.name: doc.timexs for doc in te3.test}\n",
    "\n",
    "print(\"Baseline Results:\")\n",
    "baseline_results = evaluate.timex_identification(\n",
    "    annotation_te3, \n",
    "    baseline_predictions, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"HeidelTime Results:\")\n",
    "heideltime_results = evaluate.timex_identification(\n",
    "    annotation_te3, \n",
    "    heideltime_predictions, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385152a1-fbf1-4e52-85ed-733274f3910a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that we have to build the annotation in the same format as the predictions, i.e., a dictionary with the name of the documents as key and the annotated timexs as the values.\n",
    "\n",
    "As a result of the evaluation function `tieval` produces the micro and macro metrics for the precision, recall, and F1-score. \n",
    "\n",
    "### TLink Classification\n",
    "\n",
    "For the temporal relation classification task, it adds to those metrics, the accuracy and the temporal metrics (precision, recall, and awareness) of the model. The script bellow provides the evaluation of the CogCompTime2 system on the TCR corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2680b-745b-4942-8b66-acf48ae708ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       |   accuracy |    f1 |   precision |   recall |   temporal_awareness |   temporal_precision |   temporal_recall |\n",
      "|-------+------------+-------+-------------+----------+----------------------+----------------------+-------------------|\n",
      "| macro |      0.748 | 0.748 |       0.748 |    0.748 |                0.687 |                0.74  |             0.641 |\n",
      "| micro |      0.753 | 0.753 |       0.753 |    0.753 |                0.693 |                0.742 |             0.65  |\n"
     ]
    }
   ],
   "source": [
    "annotation_tcr = {doc.name: doc.tlinks for doc in tcr.documents}\n",
    "cct2_result = evaluate.tlink_classification(\n",
    "    annotation_tcr, \n",
    "    cct2_predictions,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7cc6a-af11-4773-911a-54cfba23b3db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This finalizes the overview of `tieval`. With this introduction one should be familiar with the basic functionality of the framework which can empower the development of temporal information extraction system, and produce a comprehensive evaluation of it, on the several corpus the framework provides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
